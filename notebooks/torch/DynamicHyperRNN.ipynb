{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tutorial adapted from https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 ['Portuguese', 'Czech', 'Korean', 'Arabic', 'English', 'Russian', 'German', 'Spanish', 'Vietnamese', 'Polish', 'Irish', 'Japanese', 'French', 'Scottish', 'Greek', 'Chinese', 'Italian', 'Dutch']\n",
      "O'Neal\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    with open(filename, encoding='utf-8') as some_file:\n",
    "        return [unicodeToAscii(line.strip()) for line in some_file]\n",
    "\n",
    "# Build the category_lines dictionary, a list of lines per category\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "for filename in findFiles('../names/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "if n_categories == 0:\n",
    "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
    "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
    "        'the current directory.')\n",
    "\n",
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicodeToAscii(\"O'Néàl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TargetRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TargetRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.rnn = nn.GRUCell(n_categories + input_size, self.hidden_size)\n",
    "        self.linear = nn.Linear(self.hidden_size, 256)\n",
    "        self.linear2 = nn.Linear(256, 256)\n",
    "        self.linear3 = nn.Linear(256, self.output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, inp, hidden):\n",
    "        concatenated = torch.cat((category, inp), dim=-1)\n",
    "        hidden =  self.rnn(concatenated, hidden)\n",
    "\n",
    "        output = self.linear(hidden)\n",
    "        output = torch.relu(output)\n",
    "        output = self.linear2(output)\n",
    "        output = torch.relu(output)\n",
    "        output = self.linear3(output)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random category and random line from that category\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line\n",
    "\n",
    "# One-hot vector for category\n",
    "def categoryTensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "# Make category, input, and target tensors from a random category, line pair\n",
    "def randomTrainingExample():\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor = categoryTensor(category)\n",
    "    input_line_tensor = inputTensor(line)\n",
    "    target_line_tensor = targetTensor(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train_static_hyper_rnn_step(static_hyper_rnn, optimizer, category_tensor, input_line_tensor, target_line_tensor):\n",
    "    target_line_tensor = target_line_tensor.unsqueeze(-1).to(static_hyper_rnn.device)\n",
    "    hidden = target_network.initHidden().to(static_hyper_rnn.device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    generated_params, embedding_module_output, weight_generator_output = static_hyper_rnn.generate_params()\n",
    "\n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        out = static_hyper_rnn(inp=(category_tensor.to(static_hyper_rnn.device), input_line_tensor[i].to(static_hyper_rnn.device), hidden), generated_params=generated_params, has_aux=False)\n",
    "        output, hidden = out\n",
    "        l = criterion(output, target_line_tensor[i])\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(static_hyper_rnn.parameters(), 10.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # grad_dict = {}\n",
    "    # for n, W in static_hyper_rnn.named_parameters():\n",
    "    #     if W.grad is not None:\n",
    "    #         grad_dict[\"{}_grad\".format(n)] = float(torch.sum(W.grad).item())\n",
    "\n",
    "    # for p in rnn.parameters():\n",
    "    #     p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, {\"loss\":loss.item() / input_line_tensor.size(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "def get_tensorboard_logger(\n",
    "    experiment_name: str, base_log_path: str = \"tensorboard_logs\"\n",
    "):\n",
    "    log_path = \"{}/{}_{}\".format(base_log_path, experiment_name, datetime.now())\n",
    "    train_writer = SummaryWriter(log_path, flush_secs=10)\n",
    "    full_log_path = os.path.join(os.getcwd(), log_path)\n",
    "    print(\n",
    "        \"Follow tensorboard logs with: python -m tensorboard.main --logdir '{}'\".format(full_log_path)\n",
    "    )\n",
    "    return train_writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def train(hypernet, train_iter_fn, lr, n_iters):\n",
    "    writer = get_tensorboard_logger(\"HyperRNN\")\n",
    "    optimizer = torch.optim.Adam(hypernet.parameters(), lr=lr)\n",
    "    bar = tqdm(np.arange(n_iters))\n",
    "\n",
    "    for i in bar:\n",
    "        category_tensor, input_line_tensor, target_line_tensor = randomTrainingExample()\n",
    "\n",
    "        start_time = dt.datetime.today().timestamp()\n",
    "\n",
    "        _, metrics = train_iter_fn(hypernet, optimizer, category_tensor, input_line_tensor, target_line_tensor)\n",
    "\n",
    "        time_diff = (dt.datetime.today().timestamp() - start_time) + 1e-5\n",
    "\n",
    "        metrics[\"diff\"] = time_diff\n",
    "        for key in metrics:\n",
    "            writer.add_scalar(key, metrics[key], i)\n",
    "\n",
    "\n",
    "        loss = metrics['loss']\n",
    "        bar.set_description('Loss: {} Iters p sec: {}'.format(loss, str(time_diff)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypernetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypernn.torch.hypernet import TorchHyperNetwork\n",
    "from hypernn.torch.weight_generator import TorchWeightGenerator, DefaultTorchWeightGenerator\n",
    "from hypernn.torch.embedding_module import TorchEmbeddingModule, DefaultTorchEmbeddingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193467"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_network = TargetRNN(n_letters, 128, n_letters)\n",
    "pytorch_total_params = sum(p.numel() for p in target_network.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, Optional, Any\n",
    "\n",
    "class CustomWeightGenerator(TorchWeightGenerator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_embeddings: int,\n",
    "        hidden_dim: int,\n",
    "        target_input_shape: Optional[Any] = None,\n",
    "    ):\n",
    "        super().__init__(embedding_dim, num_embeddings, hidden_dim, target_input_shape)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 32)\n",
    "        self.linear2 = nn.Linear(32, hidden_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        embedding_module_output: Dict[str, torch.Tensor],\n",
    "        inp: Iterable[Any] = [],\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        embedding = embedding_module_output[\"embedding\"]\n",
    "        x = self.linear1(embedding)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return {\"params\": x.view(-1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 8\n",
    "NUM_EMBEDDINGS = 96\n",
    "\n",
    "embedding_module = DefaultTorchEmbeddingModule.from_target(target_network, EMBEDDING_DIM, NUM_EMBEDDINGS)\n",
    "weight_generator = CustomWeightGenerator.from_target(target_network, EMBEDDING_DIM, NUM_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67592"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernetwork = TorchHyperNetwork.from_target(\n",
    "                                target_input_shape=((1, n_categories), (1, n_letters), (1, 128)),\n",
    "                                target_network=target_network,\n",
    "                                embedding_module=embedding_module,\n",
    "                                weight_generator=weight_generator\n",
    "                            )\n",
    "pytorch_total_params = sum(p.numel() for p in hypernetwork.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Hyper??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 8\n",
    "NUM_EMBEDDINGS = 512\n",
    "\n",
    "hyper_embedding_module = DefaultTorchEmbeddingModule.from_target(hypernetwork, EMBEDDING_DIM, NUM_EMBEDDINGS)\n",
    "hyper_weight_generator = CustomWeightGenerator.from_target(hypernetwork, EMBEDDING_DIM, NUM_EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8781"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_hypernetwork = TorchHyperNetwork.from_target(\n",
    "                                target_input_shape=((1, n_categories), (1, n_letters), (1, 128)),\n",
    "                                target_network=hypernetwork,\n",
    "                                embedding_module=hyper_embedding_module,\n",
    "                                weight_generator=hyper_weight_generator\n",
    "                            )\n",
    "pytorch_total_params = sum(p.numel() for p in hyper_hypernetwork.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([68229])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_hypernetwork.generate_params()[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((tensor([[ -649.3911,  -611.5127,  -950.4014,  -534.3248,  -727.5070,  -543.0729,\n",
       "             -871.5853, -2045.7465,  -615.8828,  -587.3329,  -919.3077, -1086.2100,\n",
       "             -690.4105,  -746.5254,  -510.6604, -2045.7465,  -639.7463,  -595.9708,\n",
       "             -878.9811,  -712.8958,  -697.2551,  -687.9094,  -862.0187,  -727.8147,\n",
       "             -755.9984,  -470.4913,  -819.2798,  -677.2112,  -651.0266,  -275.6285,\n",
       "             -616.3644,  -426.7748,  -471.3749,  -462.2440,  -897.6613,  -452.0502,\n",
       "             -588.6506,  -499.1289, -1060.0698,  -405.1149,  -731.4218, -2045.7465,\n",
       "            -2045.7465,  -692.6770,  -421.1514,  -553.2266,  -535.0917, -2045.7465,\n",
       "             -435.7993,  -482.9537,  -850.2637,  -470.8124,     0.0000,  -296.4098,\n",
       "             -417.1515,  -412.8741,  -544.8713,  -241.2550, -2045.7465]],\n",
       "          grad_fn=<LogSoftmaxBackward0>),\n",
       "   tensor([[ 0.0821,  0.1779, -0.1651,  0.0446,  0.2029,  0.0745,  0.0324,  0.1391,\n",
       "             0.0675, -0.2195, -0.0568, -0.2971,  0.0738, -0.0336,  0.2550, -0.0361,\n",
       "            -0.1792,  0.4790, -0.0712,  0.1567,  0.1925,  0.1110, -0.1016, -0.0322,\n",
       "             0.1612, -0.1450,  0.2199,  0.1731,  0.1372,  0.2222, -0.1645, -0.1169,\n",
       "            -0.0144,  0.3135,  0.0563,  0.2141,  0.0933, -0.2267, -0.2595,  0.2267,\n",
       "             0.0112,  0.1103,  0.3322,  0.0126, -0.1554, -0.2082, -0.1598, -0.2177,\n",
       "             0.3099,  0.0492,  0.0380, -0.0459, -0.0792,  0.1970, -0.1420,  0.1491,\n",
       "             0.2978, -0.0812,  0.0858,  0.0023, -0.3594,  0.1659,  0.2239,  0.1009,\n",
       "             0.3778,  0.2638,  0.0882, -0.1011, -0.2029, -0.4027,  0.0702,  0.4353,\n",
       "             0.0826, -0.0577,  0.1142, -0.0450, -0.4946, -0.1542,  0.1019,  0.1256,\n",
       "            -0.0635,  0.0754, -0.2488,  0.4851,  0.0464,  0.0664,  0.3492,  0.0984,\n",
       "            -0.2449, -0.3030, -0.0164,  0.0390, -0.0159,  0.1320,  0.0202,  0.0191,\n",
       "             0.0363, -0.0551, -0.0262,  0.1094,  0.2139,  0.1700,  0.3390, -0.0320,\n",
       "            -0.1687, -0.2124, -0.2696,  0.0960,  0.2560,  0.2143, -0.0025,  0.0638,\n",
       "             0.0125,  0.0359,  0.3752, -0.3026,  0.2879,  0.3699,  0.1591,  0.2248,\n",
       "             0.1049,  0.0408,  0.2121,  0.2566,  0.1887,  0.2687, -0.1656,  0.0580]],\n",
       "          grad_fn=<AddBackward0>)),\n",
       "  tensor([-0.0098,  0.1963,  0.3538,  ...,  0.3418, -0.7070,  0.0805],\n",
       "         grad_fn=<ViewBackward0>),\n",
       "  {'embedding': tensor([[ 5.5018e-01, -2.1790e-01, -4.4980e-02,  4.5117e-01, -1.3656e-01,\n",
       "            -5.6071e-01, -5.7380e-01,  4.6628e-01],\n",
       "           [ 3.0361e-02,  4.1189e-01,  1.9467e-01, -1.7491e-01,  1.7347e-01,\n",
       "            -5.8609e-01, -5.4794e-02,  1.0205e-01],\n",
       "           [ 1.0299e-01, -4.9545e-01, -2.9628e-01,  4.0355e-01, -7.0288e-01,\n",
       "            -4.7553e-01,  7.2953e-01,  1.3018e-01],\n",
       "           [ 1.4632e-02,  3.1344e-01, -2.7666e-01, -3.7383e-01,  1.3250e-01,\n",
       "             3.8975e-01, -4.9839e-01,  5.3776e-01],\n",
       "           [-2.2412e-01, -4.2618e-01,  2.5761e-01, -3.0550e-01,  3.4655e-01,\n",
       "            -2.4733e-01,  2.8245e-01, -1.2451e-01],\n",
       "           [ 3.6461e-01,  3.8569e-01,  2.3289e-01, -3.6431e-01,  4.0578e-01,\n",
       "             2.8953e-01,  3.4464e-02, -2.6195e-01],\n",
       "           [-3.8351e-02, -2.0392e-01,  9.5541e-02, -7.7220e-02, -8.8506e-01,\n",
       "             1.0816e-01,  4.1236e-01,  3.1194e-01],\n",
       "           [-4.1857e-01,  4.1474e-01,  6.8137e-02, -1.8517e-01, -1.6940e-01,\n",
       "            -1.7413e-01, -3.0387e-01,  3.4991e-01],\n",
       "           [-2.7085e-02, -7.8321e-01,  3.2808e-01, -3.3240e-01,  5.4564e-01,\n",
       "            -1.8003e-01, -7.8194e-02,  1.6039e-01],\n",
       "           [-2.5348e-01,  2.1667e-01, -8.5989e-02, -4.9758e-01,  8.3530e-01,\n",
       "            -3.8046e-01,  6.5315e-01,  6.4418e-01],\n",
       "           [ 2.1916e-01,  1.8645e-01,  3.1875e-02, -6.2651e-01,  2.9908e-01,\n",
       "            -7.4744e-02,  2.5736e-01, -6.3528e-01],\n",
       "           [ 2.1932e-01, -2.9859e-01, -3.3848e-02,  6.5446e-01,  5.4312e-01,\n",
       "             3.4758e-01,  2.3018e-01,  1.8498e-01],\n",
       "           [-3.3583e-01,  4.8552e-02, -6.9447e-01, -2.4833e-01,  2.8813e-02,\n",
       "            -1.0628e-01, -1.8330e-01,  1.5897e-01],\n",
       "           [ 3.5285e-01,  4.2517e-01,  1.6873e-01,  6.4494e-02, -2.4687e-02,\n",
       "            -2.4029e-01, -1.5860e-01, -4.0900e-02],\n",
       "           [ 3.5168e-01,  2.6410e-01,  5.7863e-02, -5.1391e-02, -4.3365e-03,\n",
       "             6.8530e-01, -3.9505e-01, -5.1154e-01],\n",
       "           [ 3.8510e-01,  8.5561e-02, -1.2996e-01,  1.4771e-01, -4.9460e-03,\n",
       "            -4.2176e-01,  1.1047e-01,  3.0622e-01],\n",
       "           [-9.3310e-02, -2.2500e-01,  2.3096e-01,  2.9248e-01, -6.5362e-02,\n",
       "             3.5917e-01, -5.1534e-02,  4.2989e-01],\n",
       "           [-7.9317e-02,  1.8752e-01, -2.9217e-01, -1.4083e-01, -1.0833e-01,\n",
       "             4.3832e-02,  4.7065e-01,  2.0598e-01],\n",
       "           [-9.9681e-02, -4.8677e-02, -1.2625e-01, -2.8985e-01,  1.4398e-01,\n",
       "             4.6119e-01, -4.5008e-01, -5.0731e-02],\n",
       "           [ 2.7822e-01, -1.0279e-01, -2.4451e-02,  3.2677e-01, -7.7991e-02,\n",
       "            -3.4247e-01, -6.5239e-02, -6.2288e-02],\n",
       "           [-1.1774e-01,  5.1350e-02, -1.6375e-01, -6.0453e-01,  1.3838e-01,\n",
       "            -1.1234e-01,  7.7019e-02,  1.5059e-01],\n",
       "           [ 2.2118e-01,  1.2132e-01, -1.9732e-01,  2.7339e-01,  1.1507e-01,\n",
       "            -7.9131e-02, -2.7427e-02,  3.3530e-02],\n",
       "           [ 1.1454e-01,  2.1151e-01,  4.2883e-01,  4.1063e-01, -1.2626e-02,\n",
       "            -1.6103e-01, -9.8261e-02,  3.1947e-02],\n",
       "           [-3.3285e-01, -2.7801e-01,  5.3594e-02,  2.9959e-01,  2.1703e-01,\n",
       "            -2.4109e-01,  3.1495e-01,  8.5606e-02],\n",
       "           [-1.8131e-02, -3.3052e-02, -5.2528e-02, -1.1886e-01,  1.7147e-01,\n",
       "            -2.5465e-01, -5.6081e-01, -4.5158e-02],\n",
       "           [-4.8658e-02,  6.1565e-02,  7.4052e-02,  3.5503e-02, -6.9511e-03,\n",
       "             1.1526e-01,  9.1244e-02, -4.6077e-02],\n",
       "           [-4.1496e-01,  5.3378e-01, -3.0650e-01,  1.7491e-01,  5.2974e-02,\n",
       "            -1.2576e-02,  1.7591e-01,  4.6094e-02],\n",
       "           [-4.3330e-02,  8.7187e-02,  1.3219e-01, -2.2454e-01, -4.7408e-01,\n",
       "             5.1979e-02, -1.3946e-01,  2.2524e-01],\n",
       "           [ 4.0507e-01,  9.5291e-02,  3.3935e-01,  2.0613e-01, -1.1543e-01,\n",
       "            -5.1140e-01,  4.5056e-01,  1.0972e-01],\n",
       "           [-8.5258e-02,  4.5683e-02, -8.1595e-03,  8.6967e-02,  2.2958e-01,\n",
       "             1.1849e-01,  2.9036e-01,  6.1321e-02],\n",
       "           [ 2.6590e-01, -1.8403e-01, -2.1991e-01,  1.6017e-01,  5.6236e-02,\n",
       "             1.9538e-01,  1.7707e-01,  3.4519e-01],\n",
       "           [-7.7174e-03, -3.0704e-01,  3.2569e-01, -3.2421e-01, -4.2752e-01,\n",
       "             2.8779e-01, -1.0729e-01, -1.7897e-01],\n",
       "           [ 6.1566e-02, -1.6315e-01, -2.5062e-01,  5.9980e-01,  2.0917e-01,\n",
       "            -3.8497e-01,  1.3043e-01,  7.3219e-02],\n",
       "           [ 3.2181e-01,  9.9973e-02,  2.2121e-01, -3.3078e-01, -2.9513e-01,\n",
       "             1.1823e-01,  1.0065e-01, -5.2338e-01],\n",
       "           [-5.0536e-02,  1.5927e-01, -4.6035e-01,  5.9815e-01,  4.8822e-01,\n",
       "            -7.9509e-05,  3.4172e-01, -1.7286e-02],\n",
       "           [-4.3486e-01, -8.3747e-02,  3.5436e-01,  2.7387e-01,  1.3660e-01,\n",
       "            -1.2262e-01,  5.1558e-02, -2.0973e-01],\n",
       "           [-5.1541e-02,  2.2166e-01,  1.8325e-02,  1.8695e-01,  3.8484e-01,\n",
       "             6.6806e-02,  8.0362e-02, -4.2557e-01],\n",
       "           [-4.2162e-01, -1.8325e-01,  3.6164e-01,  4.5762e-01,  7.1629e-02,\n",
       "             4.5214e-01, -1.5728e-02,  1.1218e-03],\n",
       "           [ 1.6941e-01,  1.2966e-01,  3.5020e-01,  4.6902e-02, -1.3053e-01,\n",
       "            -3.5581e-02, -3.5745e-01, -1.1942e-01],\n",
       "           [ 7.7620e-01,  2.2528e-01, -4.2686e-01,  1.3398e-01, -1.8529e-01,\n",
       "            -6.6253e-01, -3.1851e-01,  1.5909e-01],\n",
       "           [-6.7484e-03,  2.9480e-01,  5.1933e-03,  2.1814e-01,  6.6205e-02,\n",
       "            -3.7411e-01,  1.0810e-01, -5.3275e-01],\n",
       "           [ 5.1617e-01, -1.1440e-01, -7.1492e-01,  4.7532e-02,  1.4479e-01,\n",
       "             1.8313e-01,  4.5361e-01,  1.0752e-03],\n",
       "           [-1.8621e-01, -2.4601e-01,  4.8828e-01, -1.9046e-02,  7.3490e-02,\n",
       "             8.9965e-02,  7.5814e-02, -4.4437e-01],\n",
       "           [ 2.5589e-01, -2.6500e-01, -1.4736e-01,  1.1412e-01, -9.6289e-02,\n",
       "             3.4041e-03,  4.6467e-01,  2.6988e-01],\n",
       "           [-4.7409e-01, -1.1449e-01,  2.4753e-01, -3.3226e-01,  4.3100e-01,\n",
       "             8.6757e-02,  1.1552e-01,  7.8518e-02],\n",
       "           [ 1.7774e-02, -1.4273e-01, -3.6995e-01,  3.7231e-01,  1.9867e-01,\n",
       "            -5.5673e-02,  1.1170e-01, -2.4591e-01],\n",
       "           [-1.7726e-01,  1.1804e-01, -1.6633e-01,  1.5665e-01,  3.9783e-02,\n",
       "            -8.3273e-02, -2.6719e-01, -1.8404e-01],\n",
       "           [ 4.9608e-01, -4.9620e-02,  3.0231e-01,  1.0273e-01,  4.8415e-01,\n",
       "             4.7872e-01, -2.4459e-01, -1.3030e-02],\n",
       "           [-7.0840e-02, -4.0289e-01, -2.2575e-01,  4.6323e-02,  3.4153e-01,\n",
       "             1.3750e-01,  6.8624e-02, -2.3036e-01],\n",
       "           [ 1.2146e+00,  4.7517e-02,  2.2032e-01,  2.1327e-01, -1.7225e-01,\n",
       "             2.7827e-01,  7.6976e-01,  6.1418e-01],\n",
       "           [-2.8462e-02,  9.1514e-01, -1.8674e-01,  1.0533e-01, -5.1670e-01,\n",
       "            -9.4146e-01, -3.9387e-01,  3.4374e-01],\n",
       "           [ 3.3968e-01,  2.2911e-01, -1.5381e-01, -9.9068e-02,  6.1691e-01,\n",
       "            -5.1118e-01,  1.9018e-01,  6.6874e-01],\n",
       "           [-3.7036e-01,  2.2779e-01,  2.7997e-01, -1.0950e-01,  2.4090e-01,\n",
       "             1.6050e-01,  3.0182e-01,  2.7723e-01],\n",
       "           [ 3.9741e-01,  4.7724e-01, -3.3771e-02, -2.2255e-01,  1.8926e-01,\n",
       "            -5.9200e-01,  1.9214e-01,  1.8375e-01],\n",
       "           [-1.1057e-01,  7.6689e-02,  4.8787e-01,  2.7947e-01,  1.0990e-01,\n",
       "             2.8234e-01, -4.3525e-01, -7.9107e-01],\n",
       "           [ 5.0192e-01,  2.7116e-01,  2.7144e-01, -1.8892e-01, -2.0432e-01,\n",
       "             4.2219e-01, -1.0988e-01,  1.9154e-01],\n",
       "           [-3.9815e-01,  2.4731e-01,  7.8658e-02, -2.1930e-01, -5.9373e-01,\n",
       "             5.3762e-01,  2.9995e-01,  3.4613e-01],\n",
       "           [ 5.3406e-01, -9.8278e-02, -1.9798e-01, -3.6048e-01, -6.2480e-02,\n",
       "            -5.1347e-01,  5.0172e-01, -2.8343e-01],\n",
       "           [-7.5906e-01, -1.5104e-01,  5.4666e-01, -5.2632e-01, -5.3948e-01,\n",
       "             2.8557e-01, -3.9811e-01,  3.4567e-01],\n",
       "           [-6.7452e-01,  4.0274e-02, -1.1600e+00,  7.4057e-01,  5.1947e-01,\n",
       "             1.5322e-03, -5.7103e-01,  3.7751e-01],\n",
       "           [ 5.3576e-01, -2.4042e-01,  1.2993e-01, -4.6948e-01,  3.3722e-02,\n",
       "            -6.6941e-01, -2.2079e-01, -7.0180e-02],\n",
       "           [-5.9077e-02,  3.3846e-02,  5.2226e-01, -3.0621e-01,  7.4676e-01,\n",
       "             2.9601e-01, -1.2950e-01, -7.2763e-01],\n",
       "           [ 3.0047e-01,  4.2084e-01, -1.1408e-01, -1.3837e-02, -2.3314e-02,\n",
       "             2.7279e-01,  3.7658e-01,  7.1507e-01],\n",
       "           [ 8.3546e-01,  1.4583e-01,  8.7203e-02, -5.7980e-01, -3.2039e-01,\n",
       "            -1.8527e-01,  1.0332e-01,  6.6242e-02],\n",
       "           [-1.0943e-01,  5.6635e-01,  5.0461e-01, -1.4821e-01,  5.5003e-01,\n",
       "            -2.7484e-01,  2.8876e-01,  2.1056e-01],\n",
       "           [ 5.5676e-02, -8.3203e-02, -5.4815e-02,  1.4437e-01, -4.8478e-01,\n",
       "             3.6921e-01,  1.8695e-01, -4.2698e-02],\n",
       "           [ 7.3097e-02,  2.5299e-01,  6.0050e-01, -2.3853e-01,  5.1674e-01,\n",
       "            -4.0068e-01,  1.6805e-01,  3.6389e-01],\n",
       "           [-1.4909e-01, -3.4333e-01, -4.3203e-01,  1.8237e-01,  7.2112e-02,\n",
       "             3.9763e-01,  1.9699e-01, -2.8274e-01],\n",
       "           [ 1.7973e-01, -4.7197e-01, -1.2765e-01,  2.7316e-01,  2.5925e-01,\n",
       "            -5.2593e-01, -3.6608e-01,  2.9962e-01],\n",
       "           [-5.6944e-01, -3.7317e-01,  7.7710e-01,  5.9231e-02, -3.6694e-02,\n",
       "             4.7831e-02, -9.0434e-03, -1.6866e-01],\n",
       "           [ 2.4860e-01,  4.5291e-01, -6.7636e-01,  5.6536e-01,  4.0700e-02,\n",
       "            -2.8264e-01,  6.8549e-02, -2.4646e-01],\n",
       "           [ 2.9166e-01, -1.6865e-01,  1.2986e-01, -2.9448e-01,  7.9951e-02,\n",
       "             1.4830e-01,  2.0755e-01, -2.3046e-01],\n",
       "           [ 3.8073e-01,  2.3514e-01,  2.3347e-02, -2.2843e-01, -1.5572e-01,\n",
       "            -1.3853e-01,  4.2923e-02, -1.1914e-01],\n",
       "           [-8.0342e-01, -1.3159e-01,  3.0618e-01,  5.3366e-01, -2.3681e-01,\n",
       "             3.1732e-01, -6.9201e-02, -3.1028e-02],\n",
       "           [-1.2282e-01,  1.1887e-02, -3.7728e-01,  3.8145e-01,  7.1769e-02,\n",
       "            -1.0039e+00,  1.0264e-01, -2.9923e-01],\n",
       "           [ 2.1726e-01, -1.0672e-01,  1.7214e-01, -6.8690e-02, -1.8096e-01,\n",
       "             3.3033e-01, -2.3265e-01, -6.2710e-01],\n",
       "           [ 7.3273e-01, -2.2078e-01,  5.2596e-01,  4.0802e-01,  2.5915e-01,\n",
       "             1.4950e-01, -2.4228e-01, -4.1719e-01],\n",
       "           [ 2.3760e-01, -1.1139e-01,  9.7604e-02, -9.3555e-01,  2.4470e-02,\n",
       "            -2.0476e-01, -2.8655e-02,  6.5746e-01],\n",
       "           [ 2.4980e-01,  2.3885e-01,  3.0951e-01, -1.2662e-01, -5.0179e-01,\n",
       "             2.8074e-01, -2.9424e-01, -2.0649e-01],\n",
       "           [ 3.7601e-02,  4.2647e-02, -9.6780e-02,  2.9220e-01,  5.3248e-01,\n",
       "             3.7940e-01,  2.0182e-01,  2.0833e-01],\n",
       "           [-4.3417e-01, -3.1509e-01, -5.7847e-02, -8.3736e-02,  2.3088e-01,\n",
       "             3.8873e-01,  1.8274e-01,  1.7082e-01],\n",
       "           [-3.2333e-02,  5.5341e-01, -4.6825e-01, -3.7179e-01,  4.0377e-01,\n",
       "             1.6300e-01, -9.5965e-02,  1.6852e-01],\n",
       "           [ 4.8391e-02, -5.1073e-01,  2.4138e-01,  3.5503e-01, -2.0238e-01,\n",
       "            -2.5310e-01,  4.2959e-01,  2.3467e-01],\n",
       "           [-2.9684e-01,  1.8505e-01, -3.9927e-01,  5.9466e-02,  1.3219e-01,\n",
       "            -5.9872e-03, -5.4535e-01, -1.3285e-01],\n",
       "           [-3.2816e-01, -3.9518e-01,  3.9985e-01, -1.0339e-01,  1.9386e-01,\n",
       "            -1.4750e-01,  2.1426e-01, -1.1194e-01],\n",
       "           [ 5.2460e-02,  1.1180e-01, -1.8157e-02,  2.4103e-01,  7.4426e-02,\n",
       "             2.3204e-02, -1.4860e-01,  2.1486e-01],\n",
       "           [ 3.5095e-01,  1.8385e-02,  2.6814e-01, -4.3761e-02,  2.4444e-02,\n",
       "             6.1499e-04, -3.1397e-01, -2.4575e-01],\n",
       "           [-1.2270e-01,  8.3136e-02, -1.2665e-01, -6.7867e-02,  1.3221e-01,\n",
       "             1.7463e-01,  2.1803e-02,  4.0912e-01],\n",
       "           [-1.0718e-01,  2.1431e-01, -4.8222e-02,  2.4274e-01, -1.6525e-01,\n",
       "            -1.0221e-01, -2.4333e-02,  2.4631e-01],\n",
       "           [ 1.4556e-01,  1.3957e-01, -1.9434e-01,  1.7812e-01, -2.2657e-01,\n",
       "            -1.5990e-01, -5.3466e-02,  9.6836e-02],\n",
       "           [ 1.7033e-01,  1.3773e-01,  2.6978e-01,  1.7219e-01, -3.9190e-02,\n",
       "            -1.5964e-01, -1.5036e-01, -4.6650e-03],\n",
       "           [ 1.5851e-01, -3.6351e-01, -1.2165e-01,  1.9843e-02,  9.9896e-02,\n",
       "             9.4101e-02, -2.3820e-01, -3.0938e-03],\n",
       "           [ 1.8389e-02,  1.1347e-01,  1.2176e-01,  1.4551e-01, -1.3225e-01,\n",
       "             2.0921e-01, -1.4897e-01,  2.2604e-01],\n",
       "           [ 3.0061e-01,  1.1799e-01, -8.9261e-02,  3.1901e-02, -5.1128e-02,\n",
       "             2.0838e-01,  9.2830e-03, -2.1581e-01],\n",
       "           [-1.4406e-01,  3.8184e-01, -4.1848e-01,  1.4321e-01,  2.4625e-01,\n",
       "             9.6619e-02,  3.1141e-01,  3.2657e-01],\n",
       "           [-1.2061e-01, -2.8826e-01,  1.6468e-01,  1.6707e-01, -1.8922e-02,\n",
       "             3.9351e-01, -1.6476e-03,  1.0116e-02],\n",
       "           [ 1.7084e-01, -2.3303e-01,  1.4422e-01,  7.6076e-02, -3.2876e-02,\n",
       "            -9.4771e-02, -2.8870e-01, -2.5349e-01]], grad_fn=<EmbeddingBackward0>)},\n",
       "  {'params': tensor([-0.0098,  0.1963,  0.3538,  ...,  0.3418, -0.7070,  0.0805],\n",
       "          grad_fn=<ViewBackward0>)}),\n",
       " tensor([ 0.5502, -0.2179, -0.0450,  ..., -0.2343,  0.0652,  0.1726],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " {'embedding': tensor([[ 0.5741, -0.8158, -0.7255,  ..., -1.1258, -2.0165, -2.5248],\n",
       "          [ 0.7379, -0.0774, -0.3044,  ...,  0.7818, -0.8071, -0.4793],\n",
       "          [-0.3070, -0.7027, -0.8941,  ...,  1.6212,  0.1354,  0.0883],\n",
       "          ...,\n",
       "          [-0.5502, -0.3911,  1.0067,  ..., -0.2171, -1.0883, -0.3528],\n",
       "          [-0.4533,  1.5402,  1.0314,  ...,  0.8562,  0.8569,  1.2735],\n",
       "          [ 0.6227, -0.7097,  0.9632,  ...,  0.3987, -0.0710, -0.1496]],\n",
       "         grad_fn=<EmbeddingBackward0>)},\n",
       " {'params': tensor([ 0.5502, -0.2179, -0.0450,  ..., -0.2343,  0.0652,  0.1726],\n",
       "         grad_fn=<ViewBackward0>)})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_hypernetwork(inp=[[torch.zeros(1, n_categories), torch.zeros(1, n_letters), torch.zeros(1, 128)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "hypernetwork = hypernetwork.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow tensorboard logs with: python -m tensorboard.main --logdir '/home/shyam/Code/hyper-nn/notebooks/torch/tensorboard_logs/HyperRNN_2022-03-28 20:59:14.018919'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.4512661525181363 Iters p sec: 0.021731839904785156: 100%|██████████| 100000/100000 [21:43<00:00, 76.74it/s]  \n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "train(hypernet=hypernetwork, train_iter_fn=train_static_hyper_rnn_step, lr=learning_rate, n_iters=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernetwork = hypernetwork.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranin\n",
      "Ushenkov\n",
      "Shananov\n",
      "Gas\n",
      "Esser\n",
      "Raster\n",
      "Santina\n",
      "Perez\n",
      "Abana\n",
      "Chan\n",
      "Hui\n",
      "Inang\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(category, start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = target_network.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            out, _, _, _ = hypernetwork(inp=(category_tensor, input[0], hidden))\n",
    "            output, hidden = out\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter))\n",
    "\n",
    "samples('Russian', 'RUS')\n",
    "\n",
    "samples('German', 'GER')\n",
    "\n",
    "samples('Spanish', 'SPA')\n",
    "\n",
    "samples('Chinese', 'CHI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic HyperNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, Tuple\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DynamicTorchEmbeddingModule(TorchEmbeddingModule):\n",
    "    def __init__(self, embedding_dim: int, num_embeddings: int, input_shape):\n",
    "        super().__init__(embedding_dim, num_embeddings)\n",
    "        self.rnn_hidden_dim = num_embeddings\n",
    "        self.gru = nn.RNNCell(np.prod(input_shape), num_embeddings)\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "    def forward(self, inp, hidden_state: Optional[torch.Tensor] = None):\n",
    "        x = torch.cat(inp[:-1], -1)\n",
    "        if hidden_state is None:\n",
    "            hidden_state = torch.zeros(x.size(0), self.rnn_hidden_dim).to(self.device)\n",
    "        hidden_state = torch.sigmoid(self.gru(x, hidden_state))\n",
    "        indices = torch.arange(self.num_embeddings).to(self.device)\n",
    "        embedding = self.embedding(indices)*hidden_state.view(self.num_embeddings, 1)\n",
    "        return {\"embedding\":embedding, \"hidden\":hidden_state}\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.num_embeddings).to(self.device)\n",
    "\n",
    "class DynamicTorchWeightGenerator(TorchWeightGenerator):\n",
    "    def __init__(self, embedding_dim: int, num_embeddings: int, hidden_dim: int, input_shape: Optional[Any] = None):\n",
    "        super().__init__(embedding_dim, num_embeddings, hidden_dim, input_shape)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 16)\n",
    "        self.linear2 = nn.Linear(16, hidden_dim)\n",
    "\n",
    "    def forward(\n",
    "        self, embedding_module_output: Dict[str, torch.Tensor], inp: Optional[Any] = None\n",
    "    ) -> torch.Tensor:\n",
    "        embedding = embedding_module_output[\"embedding\"]\n",
    "        x = self.linear1(embedding)\n",
    "        x = F.relu(x)\n",
    "        return {\"params\":self.linear2(x).view(-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193467"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_network = TargetRNN(n_letters, 128, n_letters)\n",
    "pytorch_total_params = sum(p.numel() for p in target_network.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 8\n",
    "NUM_EMBEDDINGS = 96\n",
    "\n",
    "dynamic_embedding_module = DynamicTorchEmbeddingModule.from_target(target_network, EMBEDDING_DIM, NUM_EMBEDDINGS, target_input_shape=(n_categories+n_letters,))\n",
    "dynamic_weight_generator = DynamicTorchWeightGenerator.from_target(target_network, EMBEDDING_DIM, NUM_EMBEDDINGS, target_input_shape=(n_categories+n_letters,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52264"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_hypernetwork = TorchHyperNetwork(\n",
    "                                target_input_shape=((1, n_categories), (1, n_letters), (1, 128)),\n",
    "                                target_network=target_network,\n",
    "                                embedding_module=dynamic_embedding_module,\n",
    "                                weight_generator=dynamic_weight_generator\n",
    "                            )\n",
    "pytorch_total_params = sum(p.numel() for p in dynamic_hypernetwork.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "dynamic_hypernetwork = dynamic_hypernetwork.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train_dynamic_hyper_rnn_step(dynamic_hyper_rnn, optimizer, category_tensor, input_line_tensor, target_line_tensor):\n",
    "    target_line_tensor = target_line_tensor.unsqueeze(-1).to(dynamic_hyper_rnn.device)\n",
    "    hidden = target_network.initHidden().to(dynamic_hyper_rnn.device)\n",
    "    hyper_hidden = dynamic_hyper_rnn.embedding_module.initHidden()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        out, _, embedding_output, _ = dynamic_hyper_rnn(inp=(category_tensor.to(dynamic_hyper_rnn.device), input_line_tensor[i].to(dynamic_hyper_rnn.device), hidden), embedding_module_kwargs={\"hidden_state\":hyper_hidden})\n",
    "        hyper_hidden = embedding_output[\"hidden\"]\n",
    "        output, hidden = out\n",
    "        l = criterion(output, target_line_tensor[i])\n",
    "        loss += l\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(dynamic_hyper_rnn.parameters(), 10.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    grad_dict = {}\n",
    "    for n, W in dynamic_hyper_rnn.named_parameters():\n",
    "        if W.grad is not None:\n",
    "            grad_dict[\"{}_grad\".format(n)] = float(torch.sum(W.grad).item())\n",
    "\n",
    "    # for p in rnn.parameters():\n",
    "    #     p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, {\"loss\":loss.item() / input_line_tensor.size(0), **grad_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow tensorboard logs with: python -m tensorboard.main --logdir '/home/shyam/Code/hyper-nn/notebooks/torch/tensorboard_logs/HyperRNN_2022-03-28 21:20:57.878494'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.272874196370443 Iters p sec: 0.018361078033447265: 100%|██████████| 100000/100000 [30:53<00:00, 53.97it/s]  \n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "train(hypernet=dynamic_hypernetwork, train_iter_fn=train_dynamic_hyper_rnn_step, lr=learning_rate, n_iters=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_hypernetwork = dynamic_hypernetwork.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rukin\n",
      "Urelin\n",
      "Shalinov\n",
      "Grot\n",
      "Etser\n",
      "Roster\n",
      "Sallan\n",
      "Pales\n",
      "Allana\n",
      "Chu\n",
      "Huan\n",
      "Ing\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Sample from a category and starting letter\n",
    "def sample(category, start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = target_network.initHidden()\n",
    "        hyper_hidden = dynamic_hypernetwork.embedding_module.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        hidden_states = []\n",
    "        for i in range(max_length):\n",
    "            out, _, embedding_output, _ = dynamic_hypernetwork(inp=(category_tensor, input[0], hidden), embedding_module_kwargs={\"hidden_state\":hyper_hidden})\n",
    "            hyper_hidden = embedding_output[\"hidden\"]\n",
    "            output, hidden = out\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "\n",
    "        return output_name\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter))\n",
    "\n",
    "samples('Russian', 'RUS')\n",
    "\n",
    "samples('German', 'GER')\n",
    "\n",
    "samples('Spanish', 'SPA')\n",
    "\n",
    "samples('Chinese', 'CHI')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d64cb66d3d902aa83000daa06ca958bef94bde318911a82aee5f8df2bb8934b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pycanvas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
