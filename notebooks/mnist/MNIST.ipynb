{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyper HYPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorbaord Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "\n",
    "# Extraction function\n",
    "def tflog2pandas(path: str) -> pd.DataFrame:\n",
    "    \"\"\"convert single tensorflow log file to pandas DataFrame\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        path to tensorflow log file\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        converted dataframe\n",
    "    \"\"\"\n",
    "    DEFAULT_SIZE_GUIDANCE = {\n",
    "        \"compressedHistograms\": 1,\n",
    "        \"images\": 1,\n",
    "        \"scalars\": 0,  # 0 means load all\n",
    "        \"histograms\": 1,\n",
    "    }\n",
    "    runlog_data = pd.DataFrame({\"metric\": [], \"value\": [], \"step\": []})\n",
    "    try:\n",
    "        event_acc = EventAccumulator(path, DEFAULT_SIZE_GUIDANCE)\n",
    "        event_acc.Reload()\n",
    "        tags = event_acc.Tags()[\"scalars\"]\n",
    "        for tag in tags:\n",
    "            event_list = event_acc.Scalars(tag)\n",
    "            values = list(map(lambda x: x.value, event_list))\n",
    "            step = list(map(lambda x: x.step, event_list))\n",
    "            r = {\"metric\": [tag] * len(step), \"value\": values, \"step\": step}\n",
    "            r = pd.DataFrame(r)\n",
    "            runlog_data = pd.concat([runlog_data, r])\n",
    "    # Dirty catch of DataLossError\n",
    "    except Exception:\n",
    "        print(\"Event file possibly corrupt: {}\".format(path))\n",
    "        traceback.print_exc()\n",
    "    return runlog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tflog2pandas(\"/home/kokkgoblin/Code/hyper-nn/notebooks/tensorboard_logs/CIFAR10_TorchTargetNetwork_2022-05-04 13:26:07.063256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"metric\"] == \"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_loss(path: str):\n",
    "    df = tflog2pandas(path)\n",
    "    return np.array(df[df[\"metric\"] == \"accuracy\"][\"value\"]), np.array(df[df[\"metric\"] == \"loss\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_acc, target_loss = get_accuracy_loss(\"/home/kokkgoblin/Code/hyper-nn/notebooks/tensorboard_logs/CIFAR10_TorchTargetNetwork_2022-05-04 13:26:07.063256\")\n",
    "\n",
    "level1_acc, level1_loss = get_accuracy_loss('/home/kokkgoblin/Code/hyper-nn/notebooks/tensorboard_logs/CIFAR10_HypernetworkTorchLEVEL1_num_embeddings64_hiddendim82557_num_params413041_iterations500_2022-05-04 13:46:54.924302')\n",
    "level2_acc, level2_loss = get_accuracy_loss('/home/kokkgoblin/Code/hyper-nn/notebooks/tensorboard_logs/CIFAR10_HypernetworkTorchLEVEL2_num_embeddings64_hiddendim11618_num_params58346_iterations500_2022-05-04 14:08:06.548585')\n",
    "level3_acc, level3_loss = get_accuracy_loss('/home/kokkgoblin/Code/hyper-nn/notebooks/tensorboard_logs/CIFAR10_HypernetworkTorchLEVEL3_num_embeddings64_hiddendim1642_num_params8466_iterations500_2022-05-04 14:29:23.346215')\n",
    "level4_acc, level4_loss = get_accuracy_loss('/home/kokkgoblin/Code/hyper-nn/notebooks/tensorboard_logs/CIFAR10_HypernetworkTorchLEVEL4_num_embeddings64_hiddendim239_num_params1451_iterations500_2022-05-04 14:50:43.351635')\n",
    "level5_acc, level5_loss = get_accuracy_loss('/home/kokkgoblin/Code/hyper-nn/notebooks/tensorboard_logs/CIFAR10_HypernetworkTorchLEVEL5_num_embeddings32_hiddendim104_num_params648_iterations500_2022-05-04 15:12:04.348970')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(level1_acc.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num target params: 5283594 \\\n",
    "Hypernetwork with level: 1, num_embeddings: 64, embedding_dim: 8, hidden_dim: 82557, num_parameters: 413041 \\\n",
    "Hypernetwork with level: 2, num_embeddings: 64, embedding_dim: 8, hidden_dim: 11618, num_parameters: 58346 \\\n",
    "Hypernetwork with level: 3, num_embeddings: 64, embedding_dim: 8, hidden_dim: 1642, num_parameters: 8466 \\\n",
    "Hypernetwork with level: 4, num_embeddings: 64, embedding_dim: 8, hidden_dim: 239, num_parameters: 1451 \\\n",
    "Hypernetwork with level: 5, num_embeddings: 32, embedding_dim: 4, hidden_dim: 104, num_parameters: 648 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x, target_acc, label = \"target\")\n",
    "plt.plot(x, level1_acc, label = \"level 1\")\n",
    "plt.plot(x, level2_acc, label = \"level 2\")\n",
    "plt.plot(x, level3_acc, label = \"level 3\")\n",
    "plt.plot(x, level4_acc, label = \"level 4\")\n",
    "plt.plot(x, level5_acc, label = \"level 5\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(x, np.log(target_loss), label = \"target\")\n",
    "plt.plot(x, np.log(level1_loss), label = \"level 1\")\n",
    "plt.plot(x, np.log(level2_loss), label = \"level 2\")\n",
    "plt.plot(x, np.log(level3_loss), label = \"level 3\")\n",
    "plt.plot(x, np.log(level4_loss), label = \"level 4\")\n",
    "plt.plot(x, np.log(level5_loss), label = \"level 5\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "## load mnist dataset\n",
    "\n",
    "root = 'data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = torchvision.datasets.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = torchvision.datasets.MNIST(root=root, train=False, transform=trans, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(train_set[i][0].detach().numpy().squeeze(), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(train_set[i][1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TorchMNISTConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 512, 3, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(512, 512, 3, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(512, 512, 3, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(512, 10, 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x).view(x.size(0), -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_target_network = TorchMNISTConvNet()\n",
    "pytorch_total_params = sum(p.numel() for p in torch_target_network.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_target_network(torch.zeros(1,1,28,28)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypernn.torch.static_hypernet import TorchHyperNetwork\n",
    "from typing import Any, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Embedding Module + Weight Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating hypernetwork with Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernetwork = TorchHyperNetwork.from_target(\n",
    "    torch_target_network,\n",
    "    embedding_dim=4,\n",
    "    num_embeddings=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "class RecursiveHyperNetwork(TorchHyperNetwork):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_network: nn.Module,\n",
    "        num_target_parameters: Optional[int] = None,\n",
    "        embedding_dim: int = 100,\n",
    "        num_embeddings: int = 3,\n",
    "        hidden_dim: Optional[int] = None,\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "                    target_network = target_network,\n",
    "                    num_target_parameters = num_target_parameters,\n",
    "                    embedding_dim = embedding_dim,\n",
    "                    num_embeddings = num_embeddings,\n",
    "                    hidden_dim = hidden_dim,\n",
    "                )\n",
    "        # self.layer_norm = nn.LayerNorm([self.hidden_dim])\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        if self.embedding_module is None:\n",
    "            self.embedding_module = nn.Embedding(\n",
    "                self.num_embeddings, self.embedding_dim\n",
    "            )\n",
    "\n",
    "        if self.weight_generator_module is None:\n",
    "            self.weight_generator_module = nn.Sequential(\n",
    "                nn.Linear(\n",
    "                    self.embedding_dim, self.hidden_dim\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def weight_generator(self, embedding: torch.Tensor) -> torch.Tensor:\n",
    "        return self.weight_generator_module(embedding / embedding.max()).view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_hyper(num_levels: int, target_network, embedding_dim, num_embeddings):\n",
    "    t = target_network\n",
    "    for i in range(num_levels - 1):\n",
    "        hypernetwork = RecursiveHyperNetwork.from_target(\n",
    "                    t,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_embeddings=num_embeddings\n",
    "        )\n",
    "        t = hypernetwork\n",
    "    hypernetwork = RecursiveHyperNetwork.from_target(\n",
    "                t,\n",
    "                embedding_dim=4,\n",
    "                num_embeddings=num_embeddings\n",
    "    )\n",
    "    return hypernetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperHyper(nn.Module):\n",
    "    def __init__(self, num_levels, target_network, embedding_dim, num_embeddings):\n",
    "        super().__init__()\n",
    "        self.num_levels = num_levels\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.hypernetwork = hyper_hyper(num_levels, target_network, embedding_dim, num_embeddings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x.to(self.hypernetwork.device)\n",
    "        for i in range(self.num_levels - 1):\n",
    "                inp = [inp]\n",
    "        return self.hypernetwork(inp=[inp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train hypernetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "def get_tensorboard_logger(\n",
    "    experiment_name: str, base_log_path: str = \"tensorboard_logs\"\n",
    "):\n",
    "    log_path = \"{}/{}_{}\".format(base_log_path, experiment_name, datetime.now())\n",
    "    train_writer = SummaryWriter(log_path, flush_secs=10)\n",
    "    full_log_path = os.path.join(os.getcwd(), log_path)\n",
    "    print(\n",
    "        \"Follow tensorboard logs with: python -m tensorboard.main --logdir '{}'\".format(full_log_path)\n",
    "    )\n",
    "    return train_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = [\n",
    "    (8, 64),\n",
    "    (8, 64),\n",
    "    (8, 64),\n",
    "    (8, 64),\n",
    "    (4, 32),\n",
    "    (4, 32),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def make_hyperhypers(hyper_params, target):\n",
    "    hypers = []\n",
    "    pytorch_total_params = sum(p.numel() for p in target.parameters() if p.requires_grad)\n",
    "    print(\"num target params: {}\".format(pytorch_total_params))\n",
    "    for i in range(1, len(hyper_params)):\n",
    "        hyper  = HyperHyper(i, deepcopy(target), hyper_params[i-1][0], hyper_params[i-1][1])\n",
    "        pytorch_total_params = sum(p.numel() for p in hyper.parameters() if p.requires_grad)\n",
    "        print(\"Hypernetwork with level: {}, num_embeddings: {}, embedding_dim: {}, hidden_dim: {}, num_parameters: {}\".format(i, hyper.num_embeddings, hyper.embedding_dim, hyper.hypernetwork.hidden_dim, pytorch_total_params))\n",
    "        hypers.append(hyper)\n",
    "    return hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def random_sample(dataset, num_samples:int = 1):\n",
    "    length = len(dataset)\n",
    "    random_indices = np.random.randint(0,length, num_samples)\n",
    "    return torch.stack([dataset[idx][0] for idx in random_indices]), torch.tensor([dataset[idx][1] for idx in random_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network_mnist(network, name, train_set, test_set, num_iterations=500, lr = 0.01):\n",
    "    print(\"STARTING\", name)\n",
    "        \n",
    "    writer = get_tensorboard_logger(name)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=512,\n",
    "                 shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "                    dataset=test_set,\n",
    "                    batch_size=512,\n",
    "                    shuffle=False)        \n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    network = network.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "    bar = tqdm.tqdm(np.arange(num_iterations))\n",
    "\n",
    "    grad_dict = {}\n",
    "\n",
    "    for i in bar:\n",
    "\n",
    "        train_loss = []\n",
    "        # x, target = random_sample(train_set, 512)\n",
    "        for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        # for sample in samples:\n",
    "            x = x.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = network(x)\n",
    "            loss =  torch.nn.functional.cross_entropy(out, target)\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(network.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        avg_loss = np.mean(train_loss)\n",
    "\n",
    "        num_correct = 0\n",
    "        count = 0\n",
    "        for batch_idx, (x, target) in enumerate(test_loader):\n",
    "            count += x.size(0)\n",
    "            with torch.no_grad():\n",
    "                out = network(x.to(device))\n",
    "                _, predicted = torch.max(out.detach(), -1)\n",
    "            num_correct += (predicted.detach().cpu() == target.data).sum()\n",
    "        accuracy = num_correct / count\n",
    "\n",
    "        grad_dict = {}\n",
    "        for n, W in network.named_parameters():\n",
    "            if W.grad is not None:\n",
    "                grad_dict[\"{}_grad\".format(n)] = float(torch.sum(W.grad).item())\n",
    "\n",
    "        metrics = {\"loss\":loss.item(), \"accuracy\":accuracy.item(), **grad_dict}\n",
    "\n",
    "        for key in metrics:\n",
    "            writer.add_scalar(key, metrics[key], i)\n",
    "        \n",
    "        bar.set_description(\"Loss: {}, Test Acc: {}\".format(avg_loss, accuracy))\n",
    "    return network, loss.item(), accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hyper_hyper(hypernetworks, torch_target_network, train_set, test_set, dataset_name=\"MNIST\", num_iterations = 500, lr: float = 0.01, train_target = True):\n",
    "    \n",
    "    if train_target:\n",
    "        torch_target_network, loss, accuracy = train_network_mnist(torch_target_network, \"{}_TorchTargetNetwork\".format(dataset_name), train_set, test_set, num_iterations)\n",
    "        device = torch.device(\"cpu\")\n",
    "        torch_target_network = torch_target_network.to(device)\n",
    "        print(\"FINAL Metrics FOR TARGET NETWORK: Loss: {}, ACCURACY: {}\".format(loss, accuracy))\n",
    "\n",
    "        device = torch.device(\"cpu\")\n",
    "        torch_target_network = torch_target_network.to(device)\n",
    "\n",
    "    for level in range(len(hypernetworks)):\n",
    "\n",
    "        hypernetwork = hypernetworks[level]\n",
    "        pytorch_total_params = sum(p.numel() for p in hypernetwork.parameters() if p.requires_grad)\n",
    "        num_embeddings = hypernetwork.num_embeddings\n",
    "        embedding_dim = hypernetwork.embedding_dim\n",
    "        hidden_dim =  hypernetwork.hypernetwork.hidden_dim\n",
    "\n",
    "        \n",
    "        name = \"{}_HypernetworkTorchLEVEL{}_num_embeddings{}_hiddendim{}_num_params{}_iterations{}\".format(dataset_name, level+1, num_embeddings, hidden_dim, pytorch_total_params, num_iterations)\n",
    "        hypernetwork, loss, accuracy = train_network_mnist(hypernetwork, name, train_set, test_set, num_iterations, lr) \n",
    "        \n",
    "        print(\"FINAL Metrics FOR HYPERNETWORK level {}: Loss: {}, Test accuracy: {}\".format(level+1,  loss, accuracy))\n",
    "        device = torch.device(\"cpu\")\n",
    "        hypernetwork = hypernetwork.to(device)\n",
    "        hypernetworks[level] = hypernetwork\n",
    "    return torch_target_network, hypernetworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypers = make_hyperhypers(hyper_params, torch_target_network)\n",
    "# trained_target_network, hypers = train_hyper_hyper(hypers, torch_target_network, train_set, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display subset of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig = plt.figure()\n",
    "# for i in range(6):\n",
    "#     out = hypernetwork(test_set[i][0].to(device))\n",
    "#     _, predicted = torch.max(out.detach(), -1)\n",
    "#     plt.subplot(2,3,i+1)\n",
    "#     plt.imshow(test_set[i][0].detach().numpy().squeeze(), cmap='gray', interpolation='none')\n",
    "#     plt.title(\"Prediction: {}\".format(predicted.item()))\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = torchvision.datasets.FashionMNIST(root=root, train=False, transform=trans, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(train_set[i][0].detach().numpy().squeeze(), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(train_set[i][1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_target_network = TorchMNISTConvNet()\n",
    "pytorch_total_params = sum(p.numel() for p in torch_target_network.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypers = make_hyperhypers(hyper_params, torch_target_network)\n",
    "# trained_target_network, hypers = train_hyper_hyper(hypers, torch_target_network, train_set, test_set, \"FASHIONMNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.MNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = torchvision.datasets.MNIST(root=root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_target_network = TorchMNISTConvNet()\n",
    "\n",
    "# hypers = make_hyperhypers(hyper_params, torch_target_network)\n",
    "# trained_target_network, hypers = train_hyper_hyper(hypers, torch_target_network, train_set, test_set, dataset_name=\"TransferMNIST\", num_iterations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer to FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "# if not exist, download mnist dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(root=root, train=True, transform=trans, download=True)\n",
    "test_set = torchvision.datasets.FashionMNIST(root=root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_target_network, hypers = train_hyper_hyper(hypers, torch_target_network, train_set, test_set, dataset_name=\"TransferFashionMNIST\", num_iterations=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.CIFAR10(root=root, train=True, transform=trans, download=True)\n",
    "test_set = torchvision.datasets.CIFAR10(root=root, train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(1)\n",
    "\n",
    "torch_target_network = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 10,\n",
    "    dim = 256,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 512,\n",
    "    dropout = 0.0,\n",
    "    emb_dropout = 0.0\n",
    ")\n",
    "pytorch_total_params = sum(p.numel() for p in torch_target_network.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = make_hyperhypers(hyper_params, torch_target_network)\n",
    "trained_target_network, hypers = train_hyper_hyper(hypers[:3], torch_target_network, train_set, test_set, dataset_name=\"CIFAR10_5000ITERS_SEED1\", num_iterations=100, train_target = True, lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(2)\n",
    "\n",
    "torch_target_network = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 10,\n",
    "    dim = 256,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 512,\n",
    "    dropout = 0.0,\n",
    "    emb_dropout = 0.0\n",
    ")\n",
    "pytorch_total_params = sum(p.numel() for p in torch_target_network.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = make_hyperhypers(hyper_params, torch_target_network)\n",
    "trained_target_network, hypers = train_hyper_hyper(hypers[:3], torch_target_network, train_set, test_set, dataset_name=\"CIFAR10_5000ITERS_SEED2\", num_iterations=100, train_target = True, lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(100)\n",
    "\n",
    "torch_target_network = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 10,\n",
    "    dim = 256,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 512,\n",
    "    dropout = 0.0,\n",
    "    emb_dropout = 0.0\n",
    ")\n",
    "pytorch_total_params = sum(p.numel() for p in torch_target_network.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = make_hyperhypers(hyper_params, torch_target_network)\n",
    "trained_target_network, hypers = train_hyper_hyper(hypers[:3], torch_target_network, train_set, test_set, dataset_name=\"CIFAR10_5000ITERS_SEED100\", num_iterations=500, train_target = False, lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from vit_pytorch import ViT\n",
    "\n",
    "# import torch\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# torch_target_network = ViT(\n",
    "#     image_size = 32,\n",
    "#     patch_size = 4,\n",
    "#     num_classes = 10,\n",
    "#     dim = 256,\n",
    "#     depth = 4,\n",
    "#     heads = 16,\n",
    "#     mlp_dim = 512,\n",
    "#     dropout = 0.0,\n",
    "#     emb_dropout = 0.0\n",
    "# )\n",
    "# pytorch_total_params = sum(p.numel() for p in torch_target_network.parameters() if p.requires_grad)\n",
    "# pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypers = make_hyperhypers(hyper_params, torch_target_network)\n",
    "# trained_target_network, hypers = train_hyper_hyper(hypers, torch_target_network, train_set, test_set, dataset_name=\"CIFAR10\", num_iterations=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### LSTM Hypernetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Type, Union  # noqa\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMHypernetwork(TorchHyperNetwork):\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_network: nn.Module,\n",
    "        num_target_parameters: Optional[int] = None,\n",
    "        embedding_dim: int = 100,\n",
    "        num_embeddings: int = 3,\n",
    "        hidden_dim: Optional[int] = None,\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "                    target_network = target_network,\n",
    "                    num_target_parameters = num_target_parameters,\n",
    "                    embedding_dim = embedding_dim,\n",
    "                    num_embeddings = num_embeddings,\n",
    "                    hidden_dim = hidden_dim,\n",
    "                )\n",
    "\n",
    "    def make_embedding(self):\n",
    "        embedding = nn.Parameter(torch.randn(1, self.embedding_dim).to(self.device))\n",
    "        return embedding\n",
    "\n",
    "    def make_weight_generator(self):\n",
    "        return nn.GRUCell(self.embedding_dim, self.hidden_dim)\n",
    "\n",
    "    def generate_params(\n",
    "        self, inp = []\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        embedding = self.embedding\n",
    "        hidden = torch.zeros(1, self.hidden_dim, device=self.device)\n",
    "        output = []\n",
    "        for _ in range(self.num_embeddings):\n",
    "            hidden = self.weight_generator(embedding, hidden)\n",
    "            output.append(hidden)\n",
    "        return torch.stack(output).view(-1), {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernetwork = LSTMHypernetwork.from_target(\n",
    "    torch_target_network,\n",
    "    embedding_dim=8,\n",
    "    num_embeddings=7000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in hypernetwork.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernetwork(inp=[torch.zeros((1,1,28,28), device=hypernetwork.device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "hypernetwork = hypernetwork.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0,100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(dataset, num_samples:int = 1):\n",
    "    length = len(dataset)\n",
    "    random_indices = np.random.randint(0,length, num_samples)\n",
    "    return [dataset[idx] for idx in random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "num_samples = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=32,\n",
    "                 shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=32,\n",
    "                shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(hypernetwork.parameters(), lr=0.001)\n",
    "bar = tqdm.tqdm(np.arange(1000))\n",
    "\n",
    "for i in bar:\n",
    "\n",
    "    train_loss = []\n",
    "    samples = random_sample(train_set, num_samples)\n",
    "    for sample in samples:\n",
    "        x = sample[0].view(1, 1, 28, 28).to(hypernetwork.device)\n",
    "        target = torch.tensor(sample[1]).view(1,).to(hypernetwork.device)\n",
    "        optimizer.zero_grad()\n",
    "        out = hypernetwork(inp=[x.to(hypernetwork.device)], has_aux=False)\n",
    "        loss =  torch.nn.functional.cross_entropy(out.to(hypernetwork.device), target.to(hypernetwork.device))\n",
    "        print(\"SHIT\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    avg_loss = np.mean(train_loss)\n",
    "    num_correct = 0\n",
    "    count = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        count += x.size(0)\n",
    "        with torch.no_grad():\n",
    "            out = hypernetwork(inp=[x.to(hypernetwork.device)], has_aux=False)\n",
    "            _, predicted = torch.max(out.detach(), -1)\n",
    "        num_correct += (predicted.detach().cpu() == target.data).sum()\n",
    "    accuracy = num_correct / count\n",
    "    bar.set_description(\"Loss: {}, Test Acc: {}\".format(avg_loss, accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this to enable jax gpu preallocation, might lead to memory issues\n",
    "\n",
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as linen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaxMNISTConvNet(linen.Module):\n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, x):\n",
    "        x = linen.Conv(64, kernel_size=(3,3), strides=2, padding=\"VALID\")(x)\n",
    "        x = linen.relu(x)\n",
    "        x = linen.Conv(64, kernel_size=(3,3), strides=2, padding=\"VALID\")(x)\n",
    "        x = linen.relu(x)\n",
    "        x = linen.Conv(64, kernel_size=(3,3), strides=2, padding=\"VALID\")(x)\n",
    "        x = linen.relu(x)\n",
    "        x = linen.Conv(10, kernel_size=(2,2), strides=2, padding=\"VALID\")(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = linen.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_target_network = JaxMNISTConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypernn.jax.utils import count_jax_params\n",
    "\n",
    "count_jax_params(jax_target_network, inputs=[jnp.zeros((64,64,1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Jax Hypernetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, Dict\n",
    "\n",
    "from hypernn.jax.embedding_module import FlaxEmbeddingModule\n",
    "from hypernn.jax.weight_generator import FlaxWeightGenerator\n",
    "from hypernn.jax.hypernet import FlaxHyperNetwork\n",
    "\n",
    "\n",
    "class CustomFlaxEmbeddingModule(FlaxEmbeddingModule):\n",
    "    def setup(self):\n",
    "        self.embedding = linen.Embed(self.num_embeddings, self.embedding_dim)\n",
    "\n",
    "    def __call__(self, inp: Optional[Any] = None):\n",
    "        indices = jnp.arange(0, self.num_embeddings)\n",
    "        return self.embedding(indices), {}\n",
    "\n",
    "class CustomFlaxWeightGenerator(FlaxWeightGenerator):\n",
    "    def setup(self):\n",
    "        self.dense1 = linen.Dense(32)\n",
    "        self.dense2 = linen.Dense(self.hidden_dim, use_bias=False)\n",
    "\n",
    "    def __call__(self, embedding, inp: Optional[Any] = None):\n",
    "        x = self.dense1(embedding)\n",
    "        x = linen.tanh(x)\n",
    "        x = self.dense2(x)\n",
    "        return x.reshape(-1), {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making hypernetwork with `embedding_dim = 4` and `num_embeddings = 1024`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_hyper = FlaxHyperNetwork.from_target(\n",
    "    target_network=jax_target_network,\n",
    "    inputs=[jnp.zeros((28,28,1))],\n",
    "    embedding_module=CustomFlaxEmbeddingModule,\n",
    "    weight_generator=CustomFlaxWeightGenerator,\n",
    "    embedding_dim = 4,\n",
    "    num_embeddings = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_jax_params(jax_hyper, inputs=[[jnp.zeros((28,28,1))]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating train state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "\n",
    "def create_hyper_train_state(rng, model, learning_rate):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    params = model.init(rng, [jnp.zeros((28,28,1))])['params']\n",
    "    tx = optax.chain(\n",
    "        optax.adam(learning_rate)\n",
    "    )\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, params=params, tx=tx\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(logits, labels):\n",
    "    loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('apply_fn'))\n",
    "def train_step(apply_fn, state, x, targets):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = apply_fn({'params': params}, inp=[x], has_aux=False)\n",
    "        loss = cross_entropy_loss(logits=logits, labels=targets)\n",
    "        return loss, logits\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, labels=targets)\n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "bar = tqdm.tqdm(np.arange(1000))\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = create_hyper_train_state(rng, jax_hyper, 0.0002)\n",
    "\n",
    "for i in bar:\n",
    "\n",
    "    train_loss = []\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        x = rearrange(jnp.array(x.numpy()), 'b c w h -> b w h c')\n",
    "        target = jnp.array(target.numpy())\n",
    "\n",
    "        state, metrics = train_step(jax_hyper.apply, state, x, target)\n",
    "        loss = metrics[\"loss\"]\n",
    "        # optimizer.zero_grad()\n",
    "        # out = hypernetwork(inp=[x.to(hypernetwork.device)], has_aux=False)\n",
    "        # loss =  torch.nn.functional.cross_entropy(out.to(hypernetwork.device), target.to(hypernetwork.device))\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    avg_loss = np.mean(train_loss)\n",
    "    num_correct = 0\n",
    "    count = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x = rearrange(jnp.array(x.numpy()), 'b c w h -> b w h c')\n",
    "        count += x.shape[0]\n",
    "        logits = jax_hyper.apply({\"params\":state.params}, inp=[x], has_aux=False)\n",
    "        # with torch.no_grad():\n",
    "        #     out = hypernetwork(inp=[x.to(hypernetwork.device)], has_aux=False)\n",
    "        #     _, predicted = torch.max(out.detach(), -1)\n",
    "        num_correct += jnp.sum(jnp.argmax(logits, -1) == target.numpy())\n",
    "    accuracy = num_correct / count\n",
    "    bar.set_description(\"Loss: {}, Test Acc: {}\".format(avg_loss, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    out = hypernetwork(inp=[test_set[i][0].to(hypernetwork.device)], has_aux=False)\n",
    "    _, predicted = torch.max(out.detach(), -1)\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(test_set[i][0].detach().numpy().squeeze(), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Prediction: {}\".format(predicted.item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d64cb66d3d902aa83000daa06ca958bef94bde318911a82aee5f8df2bb8934b"
  },
  "kernelspec": {
   "display_name": "Python [conda env:jax_gpu] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
