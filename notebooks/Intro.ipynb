{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Quick Usage\n",
    "\n",
    "for detailed examples see [notebooks](notebooks/)\n",
    "- [Generating weights for a CNN on MNIST](notebooks/mnist/)\n",
    "- [Lunar Lander Reinforce (Vanilla Policy Gradient)](notebooks/reinforce/)\n",
    "- [Dynamic Hypernetworks for name generation](notebooks/dynamic_hypernetworks/)\n",
    "\n",
    "\n",
    "The main classes to use are `TorchHyperNetwork` and `JaxHyperNetwork` and those that inherit them. Instead of constructing them directly, use the `from_target` method, shown below. After this you can use the hypernetwork exactly like any other `nn.Module`!\n",
    "\n",
    "`hyper-nn` also makes it easy to create Dynamic Hypernetworks that use inputs to create target weights. Basic implementations (both < 100 lines) are provided with `JaxDynamicHyperNetwork` and `TorchDynamicHyperNetwork`, which use an rnn and current input to generate weights.\n",
    "\n",
    "To create hypernetworks, its easier to use the `from_target` method instead of instantiating it directly because some parameters are calculated automatically for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from hypernn.torch import TorchHyperNetwork, TorchLinearHyperNetwork, TorchDynamicHyperNetwork\n",
    "\n",
    "# any module\n",
    "target_network = nn.Sequential(\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32)\n",
    ")\n",
    "\n",
    "EMBEDDING_DIM = 4\n",
    "NUM_EMBEDDINGS = 32\n",
    "\n",
    "hypernetwork = TorchLinearHyperNetwork(\n",
    "    target_network = target_network,\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    num_embeddings = NUM_EMBEDDINGS\n",
    ")\n",
    "\n",
    "# now we can use the hypernetwork like any other nn.Module\n",
    "inp = torch.zeros((1, 32))\n",
    "\n",
    "# by default we only output what we'd expect from the target network\n",
    "output = hypernetwork(inp)\n",
    "\n",
    "# return aux_output\n",
    "output, generated_params, aux_output = hypernetwork(inp, has_aux=True)\n",
    "\n",
    "# generate params separately\n",
    "generated_params, aux_output = hypernetwork.generate_params()\n",
    "output = hypernetwork(inp, generated_params=generated_params)\n",
    "\n",
    "\n",
    "### Dynamic Hypernetwork\n",
    "\n",
    "dynamic_hypernetwork = TorchDynamicHyperNetwork(\n",
    "    input_dim = 32,\n",
    "    target_network = target_network,\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    num_embeddings = NUM_EMBEDDINGS\n",
    ")\n",
    "\n",
    "output = dynamic_hypernetwork(inp, generate_params_kwargs=dict(x=inp))\n",
    "\n",
    "# by default we only output what we'd expect from the target network\n",
    "output = dynamic_hypernetwork(inp, generate_params_kwargs=dict(x=inp, hidden_state=torch.zeros((1,32))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shyam/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/shyam/anaconda3/envs/py39/lib/python3.9/site-packages/flax/core/scope.py:740: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n",
      "  abs_value_flat = jax.tree_leaves(abs_value)\n",
      "/home/shyam/anaconda3/envs/py39/lib/python3.9/site-packages/flax/core/scope.py:741: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n",
      "  value_flat = jax.tree_leaves(value)\n"
     ]
    }
   ],
   "source": [
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "from hypernn.jax import JaxHyperNetwork, JaxLinearHyperNetwork, JaxDynamicHyperNetwork\n",
    "\n",
    "# any module\n",
    "target_network = nn.Sequential(\n",
    "    [\n",
    "        nn.Dense(64),\n",
    "        nn.relu,\n",
    "        nn.Dense(32)\n",
    "    ]\n",
    ")\n",
    "\n",
    "EMBEDDING_DIM = 4\n",
    "NUM_EMBEDDINGS = 32\n",
    "\n",
    "hypernetwork = JaxLinearHyperNetwork.from_target(\n",
    "    target_network = target_network,\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    num_embeddings = NUM_EMBEDDINGS,\n",
    "    inputs=jnp.zeros((1, 32)) # jax needs this to initialize target weights\n",
    ")\n",
    "\n",
    "# now we can use the hypernetwork like any other nn.Module\n",
    "inp = jnp.zeros((1, 32))\n",
    "key = random.PRNGKey(0)\n",
    "hypernetwork_params = hypernetwork.init(key, inp) # flax needs to initialize hypernetwork parameters first\n",
    "\n",
    "# by default we only output what we'd expect from the target network\n",
    "output = hypernetwork.apply(hypernetwork_params, inp)\n",
    "\n",
    "# return aux_output\n",
    "output, generated_params, aux_output = hypernetwork.apply(hypernetwork_params, inp, has_aux=True)\n",
    "\n",
    "# generate params separately\n",
    "generated_params, aux_output = hypernetwork.apply(hypernetwork_params, method=hypernetwork.generate_params)\n",
    "\n",
    "output = hypernetwork.apply(hypernetwork_params, inp, generated_params=generated_params)\n",
    "\n",
    "\n",
    "### Dynamic Hypernetwork\n",
    "\n",
    "dynamic_hypernetwork = JaxDynamicHyperNetwork.from_target(\n",
    "    input_dim = 32,\n",
    "    target_network = target_network,\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    num_embeddings = NUM_EMBEDDINGS,\n",
    "    inputs=jnp.zeros((1, 32)) # jax needs this to initialize target weights\n",
    ")\n",
    "dynamic_hypernetwork_params = dynamic_hypernetwork.init(key, inp, generate_params_kwargs=dict(x=inp, hidden_state=jnp.zeros((1,32)))) # flax needs to initialize hypernetwork parameters first\n",
    "\n",
    "# by default we only output what we'd expect from the target network\n",
    "output = dynamic_hypernetwork.apply(dynamic_hypernetwork_params, inp, generate_params_kwargs=dict(x=inp, hidden_state=jnp.zeros((1,32))))\n",
    "\n",
    "# by default we only output what we'd expect from the target network\n",
    "output = dynamic_hypernetwork.apply(dynamic_hypernetwork_params, inp, generate_params_kwargs=dict(x=inp, hidden_state=jnp.zeros((1,32))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Hypernetworks\n",
    "`hyper-nn` makes it easy to customize and create more complex hypernetworks.\n",
    "\n",
    "The main components to modify are the methods `generate_params`. This allows for complete control over how the hypernetwork generates parameters\n",
    "\n",
    "For example, here we extend the linear hypernetwork which uses components `embedding_module` and `weight_generator`. We implement a hypernetwork that could be useful in a multi task setting, where a one hot encoded class embedding is concatenated to every row in the embedding matrix outputted by the `embedding_module`. In addition, we override both our `make_embedding_module` and `make_weight_generator` methods to output customized modules. This whole class implementation is under 50 lines of code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Iterable, Any, Tuple, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# static hypernetwork\n",
    "from hypernn.torch import TorchHyperNetwork\n",
    "from hypernn.torch.utils import get_weight_chunk_dims\n",
    "\n",
    "class MultiTaskHypernetwork(TorchHyperNetwork):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tasks: int,\n",
    "        target_network: nn.Module,\n",
    "        num_target_parameters: Optional[int] = None,\n",
    "        embedding_dim: int = 100,\n",
    "        num_embeddings: int = 3,\n",
    "        weight_chunk_dim: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "                    target_network = target_network,\n",
    "                    num_target_parameters = num_target_parameters,\n",
    "                )\n",
    "        self.num_tasks = num_tasks\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.weight_chunk_dim = weight_chunk_dim\n",
    "        if weight_chunk_dim is None:\n",
    "            self.weight_chunk_dim = get_weight_chunk_dims(\n",
    "                self.num_target_parameters, num_embeddings\n",
    "            )\n",
    "        self.embedding_module = self.make_embedding_module()\n",
    "        self.weight_generator = self.make_weight_generator()        \n",
    "\n",
    "    def make_embedding_module(self) -> nn.Module:\n",
    "        embedding = nn.Embedding(self.num_embeddings, 8)\n",
    "        return nn.Sequential(\n",
    "            embedding,\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, self.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def make_weight_generator(self) -> nn.Module:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim + self.num_tasks, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, self.weight_chunk_dim)\n",
    "        )\n",
    "\n",
    "    def generate_params(\n",
    "        self, one_hot_task_embedding: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        embedding = self.embedding_module(\n",
    "            torch.arange(self.num_embeddings, device=self.device)\n",
    "        )\n",
    "        one_hot_task_embedding = one_hot_task_embedding.repeat(self.num_embeddings, 1) # repeat to concat to embedding\n",
    "        concatenated = torch.cat((embedding, one_hot_task_embedding), dim=-1)\n",
    "        generated_params = self.weight_generator(concatenated).view(-1)\n",
    "        return generated_params, {\"embedding\": embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "target_network = nn.Sequential(\n",
    "    nn.Linear(32, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32)\n",
    ")\n",
    "\n",
    "NUM_TASKS = 4\n",
    "EMBEDDING_DIM = 4\n",
    "NUM_EMBEDDINGS = 32\n",
    "\n",
    "hypernetwork = MultiTaskHypernetwork(\n",
    "    num_tasks = NUM_TASKS,\n",
    "    target_network = target_network,\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    num_embeddings = NUM_EMBEDDINGS\n",
    ")\n",
    "inp = torch.zeros((1, 32))\n",
    "one_hot_task_embedding = torch.tensor([0.0,0.0,1.0,0.0]).view((1,4))\n",
    "\n",
    "out = hypernetwork(inp, generate_params_kwargs=dict(one_hot_task_embedding=one_hot_task_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Using vmap for batching operations\n",
    "This is useful when dealing with dynamic hypernetworks that generate different params depending on inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shyam/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1094: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::rnn_tanh_cell. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  ret = _VF.rnn_tanh_cell(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from functorch import vmap\n",
    "\n",
    "# dynamic hypernetwork\n",
    "from hypernn.torch import TorchDynamicHyperNetwork\n",
    "\n",
    "# any module\n",
    "target_network = nn.Sequential(\n",
    "    nn.Linear(8, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 32)\n",
    ")\n",
    "\n",
    "EMBEDDING_DIM = 4\n",
    "NUM_EMBEDDINGS = 32\n",
    "\n",
    "# conditioned on input to generate param vector\n",
    "hypernetwork = TorchDynamicHyperNetwork(\n",
    "    target_network = target_network,\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    num_embeddings = NUM_EMBEDDINGS,\n",
    "    input_dim = 8\n",
    ")\n",
    "\n",
    "# batch of 10 inputs\n",
    "inp = torch.randn((10, 1, 8))\n",
    "\n",
    "# use with a for loop\n",
    "outputs = []\n",
    "for i in range(10):\n",
    "    outputs.append(hypernetwork(inp[i], generate_params_kwargs=dict(x=inp[i])))\n",
    "outputs = torch.stack(outputs)\n",
    "assert outputs.size() == (10, 1, 32)\n",
    "\n",
    "# using vmap\n",
    "from typing import Dict, Any\n",
    "\n",
    "def forward(\n",
    "    generated_params,\n",
    "    *args,\n",
    "    has_aux: bool = False,\n",
    "    assert_parameter_shapes: bool = True,\n",
    "    generate_params_kwargs: Dict[str, Any] = {},\n",
    "    **kwargs\n",
    "):\n",
    "    return hypernetwork.forward(*args,\n",
    "                                generated_params=generated_params,\n",
    "                                has_aux=has_aux,\n",
    "                                assert_parameter_shapes=assert_parameter_shapes,\n",
    "                                generate_params_kwargs=generate_params_kwargs,\n",
    "                                **kwargs)\n",
    "\n",
    "generated_vmap_params, aux_output = vmap(hypernetwork.generate_params)(inp)\n",
    "outputs = vmap(forward)(generated_vmap_params, inp)\n",
    "\n",
    "assert outputs.size() == (10, 1, 32)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e5f2d8038e6c8941d283a9a145e7dfd2f60905a23729a9c846dae091a9571f4"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py39] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
