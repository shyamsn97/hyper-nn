{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypernn.jax.embedding_module import FlaxEmbeddingModule\n",
    "from hypernn.jax.weight_generator import FlaxWeightGenerator\n",
    "from hypernn.jax.hypernet import FlaxHyperNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticEmbeddingModule(FlaxEmbeddingModule):\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Dense(self.embedding_dim, use_bias=False)\n",
    "\n",
    "    def __call__(self):\n",
    "        indices = jax.nn.one_hot(jax.numpy.arange(0,self.num_embeddings), self.num_embeddings)\n",
    "        return self.embedding(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticWeightGenerator(FlaxWeightGenerator):\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(32)\n",
    "        self.dense2 = nn.Dense(self.hidden_dim)\n",
    "\n",
    "    def __call__(self, embedding: jnp.array):\n",
    "        x = self.dense1(embedding)\n",
    "        x = nn.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(4, use_bias=False)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "import jax\n",
    "import functools\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('apply_fn'))\n",
    "def sample_actions(apply_fn, hypernetwork_params, obs):\n",
    "    out, _ =  apply_fn({'params':hypernetwork_params}, jnp.expand_dims(jnp.array(obs), 0))\n",
    "    return out\n",
    "\n",
    "def rollout(env, hypernetwork, hypernetwork_params, render=False, seed: int = 0) -> float:\n",
    "    _, target_params = hypernetwork.apply({'params':hypernetwork_params}, jnp.zeros((1,8)))\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    observations, actions, rewards, rendereds = [], [], [], []\n",
    "    while not done:\n",
    "        rendered = None\n",
    "        if render:\n",
    "            rendered = env.render(mode=\"rgb_array\")\n",
    "            rendereds.append(rendered)\n",
    "\n",
    "        out =  sample_actions(hypernetwork.apply, hypernetwork_params, np.expand_dims(obs, 0))\n",
    "        # action_logits = hypernetwork(), params=params)\n",
    "        dist = tfp.distributions.Categorical(logits=out)\n",
    "        action = dist.sample(seed=rng).item()\n",
    "        next_obs, r, done, _ = env.step(action)\n",
    "\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        rewards.append(r)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "    env.close()\n",
    "    return observations, actions, rewards, rendereds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "def get_tensorboard_logger(\n",
    "    experiment_name: str, base_log_path: str = \"tensorboard_logs\"\n",
    "):\n",
    "    log_path = \"{}/{}_{}\".format(base_log_path, experiment_name, datetime.now())\n",
    "    train_writer = SummaryWriter(log_path, flush_secs=10)\n",
    "    full_log_path = os.path.join(os.getcwd(), log_path)\n",
    "    print(\n",
    "        \"Follow tensorboard logs with: tensorboard --logdir '{}'\".format(full_log_path)\n",
    "    )\n",
    "    return train_writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "\n",
    "def create_train_state(rng, hypernet, learning_rate, input_shape, target_network):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    params = hypernet.init(rng, jnp.ones((1, 8)))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=hypernet.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('apply_fn'))\n",
    "def train_step(apply_fn, state, observations, actions, discounted_rewards):\n",
    "    def loss_fn(params):\n",
    "        logits, _ = apply_fn({'params':params}, observations)\n",
    "        dist = tfp.distributions.Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        loss = -1 * jnp.sum(discounted_rewards * log_probs)\n",
    "        return loss, logits\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(rews, gamma: float = 0.99):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + gamma*(rtgs[i + 1] if i + 1 < n else 0)\n",
    "    return rtgs\n",
    "\n",
    "def reinforce(\n",
    "        num_epochs,\n",
    "        env,\n",
    "        hypernetwork,\n",
    "        target_network,\n",
    "        lr: float = 0.0001,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    rng, init_rng = jax.random.split(rng)\n",
    "    state = create_train_state(hypernetwork, init_rng, lr, (1,8), target_network)\n",
    "\n",
    "    bar = tqdm.tqdm(np.arange(num_epochs))\n",
    "\n",
    "    for i in bar:\n",
    "        observations, actions, rewards, _ = rollout(env, hypernetwork, state.params)\n",
    "\n",
    "        discounted_rewards = discount_reward(np.array(rewards), gamma)\n",
    "        discounted_rewards = discounted_rewards - np.mean(discounted_rewards)\n",
    "        discounted_rewards = discounted_rewards / (\n",
    "            np.std(discounted_rewards) + 1e-10\n",
    "        )\n",
    "\n",
    "        observations = jnp.array(observations)\n",
    "        actions = jnp.array(actions)\n",
    "        discounted_rewards = jnp.array(discounted_rewards)\n",
    "\n",
    "        state, loss, grads = train_step(hypernetwork.apply, state, observations, actions, discounted_rewards)\n",
    "\n",
    "        # metrics = {\"loss\":loss.item(), \"rewards\":np.sum(rewards)}\n",
    "\n",
    "        bar.set_description('Loss: {}, Sum Reward: {}'.format(loss.item(), np.sum(rewards)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "target_network = MLP()\n",
    "hyper = FlaxHyperNetwork((1, 8), target_network, embedding_module_constructor=StaticEmbeddingModule, embedding_dim=32, num_embeddings=512)\n",
    "\n",
    "reinforce(10000, env, hyper, target_network)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d64cb66d3d902aa83000daa06ca958bef94bde318911a82aee5f8df2bb8934b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pycanvas')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
